{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " The Möbius Function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "In this notebook we will investigate how good small neural networks are at predicting an important number-theoretic function, and investigate other number-theoretic properties like divisibility and factorisation. We will be taking for granted a lot of the background covered in tensorflow"
      ],
      "metadata": {
        "id": "N8SQDd5uN9yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Predicting the Möbius function\n",
        "\n",
        "The [Möbius function](https://en.wikipedia.org/wiki/Möbius_function) $\\mu(n)$ indicates whether the positive integer $n$ has a squarefree factorisation into primes, and if so whether there are an odd or even number of distinct prime factors:\n",
        "\n",
        "$$\n",
        "\\mu(n) = \\begin{cases}\n",
        "-1 & \\text{ if $n$ is squarefree with an odd number of prime factors, } \\\\\n",
        "0 & \\text{ if $n$ has a repeated prime factor, and} \\\\\n",
        "+1 & \\text{ if $n$ is squarefree with an even number of prime factors.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Our goal is to train a neural net to predict the values of $\\mu$.\n",
        "\n",
        "Fix a bit-length $B \\geq 1$, then we will look at predicting $\\mu(n)$ for $n$ in the range $[0, 2^B)$, i.e. numbers of length at most $B$ when written in binary. The neural network will act as a map $\\mathbb{R}^B \\to \\mathbb{R}^3$, where the input for $n$ is a $B$-long vector of 1's and 0's (the binary representation of $n$), and the output vector in $\\mathbb{R}^3$ has the first slot corresponding to $-1$, the second to $0$, and the third to $+1$.\n",
        "\n",
        "For instance, if $B = 5$ then we are working with the numbers $n \\in \\{0, 1, \\ldots, 31\\}$. The number $n = 13$ would correspond to the input vector $[0, 1, 1, 0, 1]$, and the neural network might output $[-0.14, 0.02, 0.53]$: we would interpret these outputs using the [Softmax](https://pytorch.org/docs/stable/generated/torch.sparse.softmax.html) function to be the probability distribution $[24\\%, 29\\%, 47\\%]$, and take the maximum-likelihood result as being its classification, corresponding to the statement \"$\\mu(13)$ is predicted to be $1$\". In this case it would have gotten the wrong answer: $13$ is prime and so $\\mu(13) = -1$."
      ],
      "metadata": {
        "id": "64dqL-ToTi4y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baTDKeEIPzel"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import random\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary functions\n",
        "\n",
        "Firstly we will need to calculate $\\mu(n)$ for $n$ in a large range, which can be done using a prime sieve (and other sieving techniques). We will also need a function which outputs a big array of all the binary representations of numbers in the range $[0, 2^B)$.\n",
        "\n",
        "To find all primes in the range $[0, N)$ we will use the [Sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes). Once we have a list of primes below $N$, we can similarly use sieving techniques to calculate $\\mu(n)$ quickly for all $n \\in [0, N)$.\n",
        "\n",
        "**Tangential exercise**: using the approximation $1 + 1/2 + 1/3 + \\cdots + 1/n = O(\\log n)$ of the harmonic series, show that sieving primes up to $N$ takes $O(N \\log N)$ time. Using trial division on each number would take $1 + \\sqrt{2} + \\sqrt{3} + \\cdots + \\sqrt{N} = O(N^{3/2})$ time. If $N = 10^7$ then sieving primes using the function below takes about a second: approximately how long would it take using trial division?\n",
        "\n",
        "> The tangential exercise is unimportant from a machine learning perspective, but important if you are not used to algorithms: a better algorithm can be the difference between a program finishing in a second or finishing next week. This is especially important if you are trying to generate tons of data to feed into a neural network!"
      ],
      "metadata": {
        "id": "3VuW03z-ZQjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.lru_cache(maxsize=None)\n",
        "def primes_upto(N: int) -> List[int]:\n",
        "    \"\"\"\n",
        "    Return the prime numbers in the range [0, N) in increasing order.\n",
        "\n",
        "    >>> primes_upto(2)\n",
        "    []\n",
        "    >>> primes_upto(3)\n",
        "    [2]\n",
        "    >>> primes_upto(100)\n",
        "    [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
        "    \"\"\"\n",
        "\n",
        "    # N should be at least 2 to run the sieve below.\n",
        "    if N <= 1:\n",
        "        return []\n",
        "\n",
        "    # Sieve of Eratosthenes, using an array of integers of 0 (not prime) or 1 (prime).\n",
        "    sieve = [1] * N\n",
        "    sieve[0] = 0\n",
        "    sieve[1] = 0\n",
        "    for i in range(2, N):\n",
        "        if sieve[i] == 1:\n",
        "            for j in range(i*i, N, i):\n",
        "                sieve[j] = 0\n",
        "\n",
        "    return [p for p in range(N) if sieve[p] == 1]\n",
        "\n",
        "\n",
        "def omega_upto(N: int) -> Tuple[List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    The little-omega of n is the number of distinct prime factors of n, and the big-omega of n is the\n",
        "    number of prime factors of n counting multiplicity: https://en.wikipedia.org/wiki/Prime_omega_function.\n",
        "\n",
        "    The function omega_upto returns two parallel arrays of N elements, giving the little and big omegas of [0, N).\n",
        "\n",
        "    >>> little_omega, big_omega = omega_upto(40)\n",
        "    >>> little_omega\n",
        "    [0, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 3, 1, 1, 2, 2, 2, 2, 1, 2, 2]\n",
        "    >>> big_omega\n",
        "    [0, 0, 1, 1, 2, 1, 2, 1, 3, 2, 2, 1, 3, 1, 2, 2, 4, 1, 3, 1, 3, 2, 2, 1, 4, 2, 2, 3, 3, 1, 3, 1, 5, 2, 2, 2, 4, 1, 2, 2]\n",
        "    \"\"\"\n",
        "\n",
        "    little = [0] * N\n",
        "    big = [0] * N\n",
        "\n",
        "    for p in primes_upto(N):\n",
        "        # For each prime, mark off its multiples in the little array.\n",
        "        for i in range(p, N, p):\n",
        "            little[i] += 1\n",
        "\n",
        "        # For each prime power, mark off its multiples in the big array.\n",
        "        power = p\n",
        "        while power < N:\n",
        "            for i in range(power, N, power):\n",
        "                big[i] += 1\n",
        "            power *= p\n",
        "\n",
        "    return little, big\n",
        "\n",
        "\n",
        "def mobius_upto(N: int) -> List[int]:\n",
        "    \"\"\"\n",
        "    The mobius function of n is::\n",
        "\n",
        "        +1 if n has an even number of prime factors, all distinct,\n",
        "        -1 if n has an odd number of prime factors, all distinct, and\n",
        "         0 if n has a repeated prime factor.\n",
        "\n",
        "    We define the mobius function at 0 to be equal to 1.\n",
        "\n",
        "    >>> mobius_upto(40)\n",
        "    [1, 1, -1, -1, 0, -1, 1, -1, 0, 0, 1, -1, 0, -1, 1, 1, 0, -1, 0, -1, 0, 1, 1, -1, 0, 0, 1, 0, 0, -1, -1, -1, 0, 1, 1, 1, 0, -1, 1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    little_omega, big_omega = omega_upto(N)\n",
        "    return [\n",
        "        0 if little_omega[i] != big_omega[i] else 1 if little_omega[i] % 2 == 0 else -1\n",
        "        for i in range(N)\n",
        "    ]"
      ],
      "metadata": {
        "id": "f5rq6JMmQEOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `binary_block(B)` takes an integer $B$ and returns a tensor $T$ of shape $(2^B, B)$ such that $T[i, :]$ is the $B$-bit binary representation of $i$."
      ],
      "metadata": {
        "id": "4JPCB3fST250"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_block(bits: int):\n",
        "    \"\"\"\n",
        "    Return a tensor T of shape (2^bits, bits) such that T[i, :] is a 01-vector\n",
        "    of length bits, containing the binary representation of i. For example:\n",
        "\n",
        "    >>> binary_block(2)\n",
        "    array([[0, 0],\n",
        "           [0, 1],\n",
        "           [1, 0],\n",
        "           [1, 1]])\n",
        "    >>> binary_block(3)[6,:]\n",
        "    array([1, 1, 0])\n",
        "    \"\"\"\n",
        "    # Matrix of coordinates 100, 010, 001 etc.\n",
        "    coords = np.identity(bits, dtype=int)\n",
        "\n",
        "    # Add 0 or 1 lots of coords in every way possible.\n",
        "    result = np.zeros(bits, dtype=int)\n",
        "    for i in reversed(range(bits)):\n",
        "        result = np.row_stack([result, result + coords[:,i]])\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "3QhoIRivT16y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A dataset\n",
        "\n",
        "Fix a bit-length $B$, then our dataset will consist of $2^B$ inputs, stored in a tensor $T$ of shape $(2^B, B)$, such that $T[i, :]$ is a $B$-long vector 01 vector, and $2^B$ output categories, a $2^B$-long vector taking values in $\\{0, 1, 2\\}$. (PyTorch really wants a problem featuring $C$ categories to be labelled $[0, C)$, so we will shift the range $\\{-1, 0, 1\\}$ up to $\\{0, 1, 2\\}$ by adding $1$).\n",
        "\n",
        "The function `create_dataset(bits)` will take the bit-length `B` and return three objects:\n",
        "- A Python array `mu` containing the values of the mobius function, eg `mu[n] = \\mu(n)`.\n",
        "- A `float32` tensor `mu_input` of shape $(2^B, B)$: the inputs to the network.\n",
        "- A `long` tensor `mu_output` of shape $(2^B)$ taking values in $[0, 1, 2]$: `mu_output[n] = \\mu(n) + 1`.\n",
        "\n",
        "> *Note:* The exponentiation operator is written as `**` in Python, so $a^b$ is written `a ** b`."
      ],
      "metadata": {
        "id": "059ZpZiIZs82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(bits: int) -> Tuple[int, int, np.array, np.array]:\n",
        "    mu = mobius_upto(2**bits)\n",
        "    mu_input = torch.tensor(binary_block(bits), dtype=torch.float32)\n",
        "    mu_output = torch.tensor(mu) + 1\n",
        "\n",
        "    return mu, mu_input, mu_output\n",
        "  \n",
        "create_dataset(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEfdgFEepnnX",
        "outputId": "66a4e32e-7dcc-4cbf-f1ec-e33cb1cd088a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1, 1, -1, -1, 0, -1, 1, -1], tensor([[0., 0., 0.],\n",
              "         [0., 0., 1.],\n",
              "         [0., 1., 0.],\n",
              "         [0., 1., 1.],\n",
              "         [1., 0., 0.],\n",
              "         [1., 0., 1.],\n",
              "         [1., 1., 0.],\n",
              "         [1., 1., 1.]]), tensor([2, 2, 0, 0, 1, 0, 2, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network structure\n",
        "\n",
        "We will have a flexible network structure, requiring that there is an input layer of size $B$, an output layer of size $3$, and zero or more *hidden layers*. Every layer is an affine function, with all but the last layer followed by the [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation function.\n",
        "\n",
        "The `create_network` function is a helper for making these kind of simple networks."
      ],
      "metadata": {
        "id": "BZL0xbvub_-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network(layers: List[int]):\n",
        "    return nn.Sequential(*[\n",
        "        module\n",
        "        for x, y in zip(layers, layers[1:])\n",
        "        for module in [nn.Linear(x, y), nn.ReLU()]\n",
        "    ][:-1])\n",
        "\n",
        "\n",
        "# An example network (not the one we will end up using!)\n",
        "create_network([8, 50, 20, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh2xsrGKba0S",
        "outputId": "82dca103-b8af-4dac-f9cd-055f079e0b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=8, out_features=50, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=50, out_features=20, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=20, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        " We will be using the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), and [stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) as the training method.\n",
        "\n",
        "We will allow the training process to see 50% of the data in the whole dataset.\n",
        "\n",
        "> *Technical point 1*: Since we now have a large dataset, tracking the loss over time against the training and validation datasets becomes quite expensive and slows down the program. Hence we only calculate them every 100 epochs.\n",
        "\n",
        "> *Technical point 2*: We also use the special `with torch.no_grad():` block when running data through the network when we have no intention of performing gradient descent on the results. This block stops PyTorch from tracking auxiliary data in the `recorded_loss` tensor - we don't need this because we're never going to ask for the derivative in the `recorded_loss` tensor."
      ],
      "metadata": {
        "id": "88sMsiuhekbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the bitsize and create the dataset.\n",
        "BITS = 16\n",
        "mu, mu_input, mu_output = create_dataset(BITS)\n",
        "\n",
        "# Select 50% of the data to be training data.\n",
        "indices = torch.randperm(2**BITS)\n",
        "training_idx, validation_idx = indices[:2**BITS // 2], indices[2**BITS // 2:]\n",
        "mu_input_training, mu_output_training = mu_input[training_idx], mu_output[training_idx]\n",
        "mu_input_validation, mu_output_validation = mu_input[validation_idx], mu_output[validation_idx]\n",
        "\n",
        "def create_and_train_binary_model(hidden_layers: List[int], epochs: int):\n",
        "    model = create_network([BITS] + hidden_layers + [3])\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.01)\n",
        "\n",
        "    # We will record the loss functions across the dataset every 100 epochs.\n",
        "    LOSS_INTERVAL = 100\n",
        "    recorded_loss = torch.full(size=(epochs//LOSS_INTERVAL, 3), fill_value=0.0)\n",
        "\n",
        "    # At each epoch, show the network 10 data points and do gradient descent.\n",
        "    for epoch in range(epochs):\n",
        "        training_size = mu_input_training.shape[0]\n",
        "        batch = torch.tensor(random.sample(range(training_size), k=10))\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        model_output = model(mu_input_training[batch])\n",
        "        loss = loss_function(model_output, mu_output_training[batch])\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        if epoch % LOSS_INTERVAL == 0:\n",
        "            # Record the training and validation loss of the network at each epoch.\n",
        "            # We do this with no_grad enabled: this disables computation tracking and uses much less RAM.\n",
        "            with torch.no_grad():\n",
        "                recorded_loss[epoch // LOSS_INTERVAL, 0] = epoch\n",
        "                recorded_loss[epoch // LOSS_INTERVAL, 1] = loss_function(model(mu_input_training), mu_output_training)\n",
        "                recorded_loss[epoch // LOSS_INTERVAL, 2] = loss_function(model(mu_input_validation), mu_output_validation)\n",
        "\n",
        "    return model, recorded_loss\n",
        "\n",
        "hidden_layers = [10, 10]\n",
        "binary_model, loss = create_and_train_binary_model(hidden_layers, epochs=6000)\n",
        "\n",
        "# Plot the loss function over time.\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(loss[:, 0], loss[:, 1], label='Training loss')\n",
        "ax.plot(loss[:, 0], loss[:, 2], label='Validation loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend()\n",
        "ax.grid()\n",
        "ax.set_title(f\"Loss over time for a network with hidden layers {hidden_layers}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "mMBImjZy2yxn",
        "outputId": "9242491c-1c89-469e-a9f2-89458e2a36b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(binary_model.parameters())"
      ],
      "metadata": {
        "id": "OyNtZuBOMGTs",
        "outputId": "7910970a-7591-451e-d9b5-b5fadd9fa14f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.1937,  0.2064,  0.0450, -0.0500, -0.1303,  0.1041,  0.0646, -0.1350,\n",
              "           0.1543, -0.0598, -0.1210, -0.1336,  0.1567, -0.0100,  0.3884,  0.1897],\n",
              "         [ 0.1432, -0.0127, -0.0789,  0.1070,  0.1034,  0.0714,  0.1400,  0.0057,\n",
              "           0.0289, -0.0899,  0.1031,  0.0180,  0.0246, -0.0566, -0.5416, -0.5053],\n",
              "         [-0.2489, -0.1987,  0.1811, -0.0589, -0.1346,  0.0436,  0.0522, -0.1100,\n",
              "          -0.0569,  0.0551, -0.0348, -0.2436,  0.1455,  0.0792,  0.0576, -0.1106],\n",
              "         [ 0.0921,  0.1529,  0.1191,  0.1913,  0.0429,  0.1680, -0.0333,  0.1686,\n",
              "           0.1206,  0.2297,  0.1524, -0.0647,  0.1315,  0.1677, -0.8076, -1.0008],\n",
              "         [ 0.1998, -0.0182,  0.2128,  0.2373, -0.1147,  0.0686,  0.0613,  0.1838,\n",
              "          -0.0679, -0.0494,  0.1090,  0.0482, -0.0453, -0.1973, -0.2006, -0.0782],\n",
              "         [ 0.0739, -0.1145, -0.1756, -0.1331, -0.2405, -0.0239, -0.0278, -0.1277,\n",
              "          -0.0216, -0.1686, -0.2441, -0.2229,  0.2349,  0.0541, -0.0196, -0.0598],\n",
              "         [ 0.0261,  0.0299, -0.0093,  0.2104,  0.0364,  0.0672, -0.0334,  0.1382,\n",
              "           0.0474,  0.0984,  0.2204, -0.1403,  0.0796,  0.0934,  0.8018,  0.7766],\n",
              "         [-0.1220,  0.0812, -0.1608, -0.0399,  0.1440, -0.1967,  0.0423, -0.1744,\n",
              "           0.0935, -0.0541, -0.0168, -0.0970, -0.0087,  0.0427, -0.0187, -0.1759],\n",
              "         [ 0.1699, -0.0246,  0.1460, -0.2039,  0.1165, -0.0816,  0.0822,  0.0463,\n",
              "           0.0866,  0.0205,  0.0639, -0.2074, -0.0301,  0.0222,  0.3829, -0.1720],\n",
              "         [-0.2238,  0.0887, -0.0385,  0.0049, -0.2138,  0.0736,  0.1894,  0.0297,\n",
              "           0.0345, -0.1340, -0.0590, -0.0913, -0.1301,  0.1505, -0.1697,  0.0115]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.1160,  0.3398,  0.1630,  0.3420, -0.0951,  0.0666, -0.1114, -0.0053,\n",
              "         -0.1220, -0.1931], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0979, -0.0837, -0.2480, -0.3025,  0.2429,  0.1371,  0.4689, -0.3207,\n",
              "          -0.0240,  0.1019],\n",
              "         [-0.0203, -0.2461, -0.0332,  0.0141,  0.0134,  0.1100, -0.2133, -0.1323,\n",
              "          -0.2476, -0.2000],\n",
              "         [-0.2761,  0.3528,  0.0760,  0.8631,  0.3035,  0.1859, -0.5945, -0.1814,\n",
              "          -0.2486, -0.1056],\n",
              "         [ 0.3663, -0.3260,  0.0127, -0.2875,  0.2405,  0.2941,  0.4850, -0.2756,\n",
              "           0.3223,  0.2793],\n",
              "         [ 0.0110,  0.2226,  0.1797, -0.0103, -0.1620,  0.2114, -0.1587,  0.1748,\n",
              "          -0.2694, -0.3084],\n",
              "         [-0.0798, -0.1618,  0.1220, -0.2155, -0.0536, -0.2224, -0.0606, -0.1615,\n",
              "           0.3159, -0.2670],\n",
              "         [-0.0436,  0.6088, -0.0860,  0.8988,  0.0539,  0.1831, -0.6922, -0.1962,\n",
              "          -0.0047,  0.3169],\n",
              "         [ 0.2034,  0.2977,  0.2936, -0.2352, -0.1203, -0.2494,  0.2418, -0.2756,\n",
              "          -0.2547,  0.0412],\n",
              "         [-0.2117,  0.0950,  0.2708, -0.2663, -0.1620,  0.1141, -0.1937,  0.0050,\n",
              "          -0.2056, -0.0111],\n",
              "         [ 0.1614,  0.2522,  0.2116, -0.2492,  0.0756, -0.2997,  0.2826, -0.0900,\n",
              "          -0.0160, -0.1101]], requires_grad=True), Parameter containing:\n",
              " tensor([ 0.2487, -0.0409,  0.3347,  0.2299, -0.2853, -0.1188,  0.2756, -0.1688,\n",
              "          0.1405, -0.2902], requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.3068, -0.1086, -0.4777,  0.1515,  0.1663, -0.3021, -0.4812,  0.1021,\n",
              "          -0.1931, -0.0229],\n",
              "         [-0.3512, -0.0244,  1.0770, -0.5272,  0.0018,  0.3067,  0.9187,  0.2176,\n",
              "           0.0657,  0.1642],\n",
              "         [ 0.1648, -0.0836, -0.2244,  0.3590, -0.2915,  0.2966, -0.6351, -0.0383,\n",
              "          -0.2594, -0.2193]], requires_grad=True), Parameter containing:\n",
              " tensor([-0.0367,  0.4856, -0.0871], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The network has been trained on a little less than 50% of the total data (20 epochs, sampling 2.5% of the dataset with replacement each time). Now we check to see how good the network is at predicting the mobius function on the whole dataset. We do this by building a dataframe of triples $(n, \\mu(n), \\text{prediction})$ and then finding the percentages of $(\\mu(n), \\text{prediction})$ items."
      ],
      "metadata": {
        "id": "FXyRjgBdmUCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  results = pd.DataFrame.from_dict({\n",
        "      'n': range(2**BITS),\n",
        "      'actual': mu,\n",
        "      'predicted': (torch.max(binary_model(mu_input), dim=1).indices - 1),\n",
        "  })\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "96GlXoQ7h98m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "63f80308-3963-4ffa-e038-464b5fcf79a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e3ecdd07-2c41-4bde-b663-9e3a82ccf590\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65531</th>\n",
              "      <td>65531</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65532</th>\n",
              "      <td>65532</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65533</th>\n",
              "      <td>65533</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65534</th>\n",
              "      <td>65534</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65535</th>\n",
              "      <td>65535</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65536 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3ecdd07-2c41-4bde-b663-9e3a82ccf590')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e3ecdd07-2c41-4bde-b663-9e3a82ccf590 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e3ecdd07-2c41-4bde-b663-9e3a82ccf590');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           n  actual  predicted\n",
              "0          0       1          0\n",
              "1          1       1         -1\n",
              "2          2      -1          1\n",
              "3          3      -1         -1\n",
              "4          4       0          0\n",
              "...      ...     ...        ...\n",
              "65531  65531       1         -1\n",
              "65532  65532       0          0\n",
              "65533  65533       0          0\n",
              "65534  65534       1          0\n",
              "65535  65535       1         -1\n",
              "\n",
              "[65536 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table below shows the percentages of (actual, predicted). Here is an example table from one of my runs:\n",
        "\n",
        "| predicted | -1   | 0    | 1    |\n",
        "|-----------|------|------|------|\n",
        "| actual    |      |      |      |\n",
        "| -1        | 0.15 | 0.00 | 0.16 |\n",
        "| 0         | 0.07 | 0.25 | 0.07 |\n",
        "| 1         | 0.14 | 0.00 | 0.16 |\n",
        "\n",
        "This shows for instance that whenever the neural network predicts that $\\mu(n) = 0$, then that prediction is true almost all the time (to within a percent, because of rounding)."
      ],
      "metadata": {
        "id": "7ETbCk5H707W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = results.pivot_table(index='actual', columns='predicted', aggfunc=len, fill_value=0)\n",
        "(counts / counts.sum().sum()).round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "E3hClVvOvJAM",
        "outputId": "349d32f6-46bd-4b8c-e201-6f6a69d3c118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2bd35ed0-6541-4b97-8fa4-fc94b64de196\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">n</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>predicted</th>\n",
              "      <th>-1</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>-1</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.19</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bd35ed0-6541-4b97-8fa4-fc94b64de196')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bd35ed0-6541-4b97-8fa4-fc94b64de196 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bd35ed0-6541-4b97-8fa4-fc94b64de196');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              n            \n",
              "predicted    -1     0     1\n",
              "actual                     \n",
              "-1         0.19  0.00  0.12\n",
              " 0         0.09  0.25  0.05\n",
              " 1         0.19  0.00  0.12"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next table shows how accurately the neural network predicts the absolute value $|\\mu(n)|$: this function is $1$ if $n$ has distinct prime factors, or $0$ if $n$ has a repeated prime factor. In the same run as above I got\n",
        "\n",
        "| predicted | 0    | 1    |\n",
        "|-----------|------|------|\n",
        "| actual    |      |      |\n",
        "| 0         | 0.25 | 0.14 |\n",
        "| 1         | 0.00 | 0.61 |\n",
        "\n",
        "which shows that the network can predict the absolute value pretty well, albeit with some false positives for $|\\mu(n) = 1|$."
      ],
      "metadata": {
        "id": "RoUYWg2h84MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts_abs = np.abs(results).pivot_table(index='actual', columns='predicted', aggfunc=len, fill_value=0)\n",
        "(counts_abs / counts_abs.sum().sum()).round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "nBu85maivAaJ",
        "outputId": "b77980a3-f6b8-4087-c982-a988dae86ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-732fe1c3-5d21-4721-bbd2-6057b0adcfb6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">n</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-732fe1c3-5d21-4721-bbd2-6057b0adcfb6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-732fe1c3-5d21-4721-bbd2-6057b0adcfb6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-732fe1c3-5d21-4721-bbd2-6057b0adcfb6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              n      \n",
              "predicted     0     1\n",
              "actual               \n",
              "0          0.25  0.14\n",
              "1          0.00  0.61"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Divisibility\n",
        "\n",
        "Make a neural network to predict, given the binary representation of a $B$-bit number, whether the number is divisible by some other fixed number $D$ (say 2, or 12).\n",
        "\n",
        "The following block should start you off with the required inputs and outputs: the inputs will again be a tensor containing the binary representation of all integers in the range $[0, 2^B)$, while the output will be $0$ meaning \"indivisible\" and $1$ meaning \"divisible\".\n",
        "\n",
        "Questions to explore:\n",
        "\n",
        "1. How few parameters can you get away with? What about when $D$ is \"easy\" (for example a power of 2)?\n",
        "2. Which divisors $D$ seem to be \"hard\" for the network?\n",
        "3. By reducing $n = n_0 + 2 n_1 + 2^2 n_2 + 2^3 n_3 + \\cdots$ modulo $D$ you can come up with (sometimes simple) divisibility rules which only look at the binary digits. For simple cases like $D = 2$ or $D = 3$, do the parameters of the neural network reflect these?\n",
        "\n"
      ],
      "metadata": {
        "id": "uJJmyBnIBaje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BITS = 10\n",
        "D = 5\n",
        "inputs = binary_block(BITS)\n",
        "outputs = torch.tensor([1 if i % D == 0 else 0 for i in range(2**BITS)])\n",
        "\n",
        "# Check the first 20 outputs to make sure we get what we're expecting.\n",
        "outputs[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa85J0duBm_y",
        "outputId": "a4c07a73-3b1c-48e9-9682-f9d8c6e2b16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Residue classes\n",
        "\n",
        "Rather than classifying a number as either divisible or not divisible by $D$, instead classify a number into one of the residue classes $\\{0, 1, \\ldots, D-1\\}$. Does the network do a better job on this problem, or on divisibility?"
      ],
      "metadata": {
        "id": "yjowI_nxZBwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Möbius revisited\n",
        "\n",
        "Machine learning has historically been done by thinking very hard about a good nonlinear transformation to apply to data, then following that transform with a linear (or at least relatively simple) classifier of some kind.\n",
        "\n",
        "In number theory there is an obvious transformation to apply, which is taking a number $n = 2^{\\nu_2} 3^{\\nu_3} 5^{\\nu_5} \\cdots$ to its \"prime factorisation vector\" $[\\nu_1, \\nu_2,  \\nu_3, \\ldots]$. Let's see what happens in our example above if we were to feed this into the neural network instead. We've provided the function which builds the tensor of prime factorisations.\n",
        "\n",
        "Let $\\pi(N)$ be the number of primes in the range $[0, N)$ (slightly different from the standard definition), then the function `prime_exponents_upto` will return a tensor `T` of shape $(N, \\pi(N))$ such that $T[n, :]$ is the list of exponents appearing in the prime factorisation of $n$. It also returns a tensor of the primes, so that we can double-check the transformation worked properly."
      ],
      "metadata": {
        "id": "IUTsZPrGm13y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prime_exponents_upto(N: int):\n",
        "    \"\"\"\n",
        "    Return a tensor of shape (N, pi(N)) where pi(N) is the number of primes in the\n",
        "    range [0, N), representing each number as its prime factorisation.\n",
        "    \"\"\"\n",
        "    primes = primes_upto(N)\n",
        "    pi = len(primes)\n",
        "\n",
        "    exponents = torch.full(size=(N, pi), fill_value=0.0)\n",
        "    for i, p in enumerate(primes):\n",
        "        power = p\n",
        "        while power < N:\n",
        "            for j in range(power, N, power):\n",
        "                exponents[j, i] += 1\n",
        "            power *= p\n",
        "    \n",
        "    return torch.tensor(primes), exponents\n",
        "\n",
        "primes, exponents = prime_exponents_upto(20)\n",
        "exponents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE378XtAqWo7",
        "outputId": "d902c736-3917-4a19-c29b-204273ec6c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [2., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [3., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 2., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [2., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [4., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [1., 2., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can double-check that our function worked by trying to apply the inverse transformation. There is some tensor trickery going on here: we have a tensor $T$ of shape $(N, \\pi(N))$ containing the exponents, where $T[n, i] = \\nu_{p_i}(n)$ for $p_i$ the $i$th prime, and a tensor $P$ of shape $(\\pi(N))$ where $P[i] = p_i$. The exponentiation operator `**` follows [Numpy broadcasting rules](https://numpy.org/doc/stable/user/basics.broadcasting.html) (bookmark that link for a perusal later!) and interprets `T ** P` to be the tensor $(P\\mathtt{**}T)[i, j] = P[j]^{T[i, j]}$, in other words it applies across the last index of both shapes. We can then use `torch.prod` across the last index (`dim=-1`) of the resulting tensor to take the product.\n",
        "\n",
        "This is an example of where a large computation is written concisely and executed fast, using the language of Numpy/torch tensors."
      ],
      "metadata": {
        "id": "wMelxebrZxf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Double-check the transformation worked by inverting it.\n",
        "# Note that 0 and 1 are represented by the same vector: this should not be much of a problem.\n",
        "torch.prod(primes ** exponents, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP5YecWmtJgq",
        "outputId": "075ad55d-87ac-4025-bc98-796a7b906516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
              "        14., 15., 16., 17., 18., 19.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}